{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naka/code/egfr_binder_rd2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import click\n",
    "import torch\n",
    "import wandb\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import logging\n",
    "import lightning as L\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from egfr_binder_rd2.datamodule import SequenceDataModule\n",
    "from egfr_binder_rd2.bt import BTRegressionModule\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "seed = 42\n",
    "debug = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(debug: bool):\n",
    "    level = logging.DEBUG if debug else logging.INFO\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Logging setup complete.\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvar = 'Average_i_pAE'\n",
    "# yvar = 'Average_i_pTM'\n",
    "yvar = 'Average_pLDDT'\n",
    "yvar = 'encoded_expression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/home/naka/code/BindCraft/outputs/EGFR_single_domain/mpnn_design_stats.csv'\n",
    "df = pd.read_csv(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 'https://raw.githubusercontent.com/adaptyvbio/egfr_competition_1/refs/heads/main/results/replicate_summary.csv'\n",
    "seqs = pd.read_csv('https://raw.githubusercontent.com/adaptyvbio/egfr_competition_1/refs/heads/main/results/result_summary.csv')\n",
    "df = pd.read_csv(fp).merge(seqs[['name', 'sequence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_map = {'low': 1, 'medium': 2, 'high': 3, 'none': 0}\n",
    "df['encoded_expression'] = df['nc_adjusted_expression'].map(expression_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['name', 'sequence']).agg({'encoded_expression': 'mean'}).reset_index().rename(columns={'sequence': 'Sequence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>encoded_expression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cetuximab_scFv</td>\n",
       "      <td>QVQLKQSGPGLVQPSQSLSITCTVSGFSLTNYGVHWVRQSPGKGLE...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Razora712-sequence_10</td>\n",
       "      <td>EELKKALQALKKEYRDKQWAVVQEMLKQHAEIAKKKEAGEINEKEA...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Razora712-sequence_2</td>\n",
       "      <td>RVKELEEEAKRKADEAEELKKRIDALQAKFNELLAAAKASSDPRKS...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Razora712-sequence_3</td>\n",
       "      <td>KELEEARKKLKEEIIKEKKAIVDQELKNHAEIADLVEAGKINEKEA...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Razora712-sequence_6</td>\n",
       "      <td>EALEEALKALKAEHAKKRKAIYDELLESHSNIADKVEKGEINKEEA...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>x.rustamov-s_11_5</td>\n",
       "      <td>MPELEAFKEEFEKFMKEFKKLSEEDIKDFKENLKKKGKPVTEEDIE...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>x.rustamov-s_15_28</td>\n",
       "      <td>MKEKLNELADEAISFAKSIFGDHPSLATFTSFANSVADDLSKEDIS...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>zalavi-egfr_binder3</td>\n",
       "      <td>SEEAKELKEKAKEKLKEALEKAKEALKDAEKAAEILKKIPEAKEAL...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>zalavi-egfr_binder7</td>\n",
       "      <td>AQAAAKETIRAVLKAAAEAARKMAEEARKLAKELEKYNKEAAKHAL...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>zalavi-egfr_binder8</td>\n",
       "      <td>IDEKKKEEYESLATELNAQAKALKAQADATGSQTYANFATAASDAA...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name                                           Sequence  \\\n",
       "0           Cetuximab_scFv  QVQLKQSGPGLVQPSQSLSITCTVSGFSLTNYGVHWVRQSPGKGLE...   \n",
       "1    Razora712-sequence_10  EELKKALQALKKEYRDKQWAVVQEMLKQHAEIAKKKEAGEINEKEA...   \n",
       "2     Razora712-sequence_2  RVKELEEEAKRKADEAEELKKRIDALQAKFNELLAAAKASSDPRKS...   \n",
       "3     Razora712-sequence_3  KELEEARKKLKEEIIKEKKAIVDQELKNHAEIADLVEAGKINEKEA...   \n",
       "4     Razora712-sequence_6  EALEEALKALKAEHAKKRKAIYDELLESHSNIADKVEKGEINKEEA...   \n",
       "..                     ...                                                ...   \n",
       "197      x.rustamov-s_11_5  MPELEAFKEEFEKFMKEFKKLSEEDIKDFKENLKKKGKPVTEEDIE...   \n",
       "198     x.rustamov-s_15_28  MKEKLNELADEAISFAKSIFGDHPSLATFTSFANSVADDLSKEDIS...   \n",
       "199    zalavi-egfr_binder3  SEEAKELKEKAKEKLKEALEKAKEALKDAEKAAEILKKIPEAKEAL...   \n",
       "200    zalavi-egfr_binder7  AQAAAKETIRAVLKAAAEAARKMAEEARKLAKELEKYNKEAAKHAL...   \n",
       "201    zalavi-egfr_binder8  IDEKKKEEYESLATELNAQAKALKAQADATGSQTYANFATAASDAA...   \n",
       "\n",
       "     encoded_expression  \n",
       "0                   1.0  \n",
       "1                   3.0  \n",
       "2                   3.0  \n",
       "3                   3.0  \n",
       "4                   2.0  \n",
       "..                  ...  \n",
       "197                 3.0  \n",
       "198                 3.0  \n",
       "199                 3.0  \n",
       "200                 3.0  \n",
       "201                 3.0  \n",
       "\n",
       "[202 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=[yvar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 144/144 [00:00<00:00, 6744.53 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 4519.15 examples/s]\n",
      "Map: 100%|██████████| 41/41 [00:00<00:00, 6234.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create and setup the DataModule\n",
    "data_module = SequenceDataModule(\n",
    "    df, \n",
    "    tokenizer_name=\"facebook/esm2_t33_650M_UR50D\",\n",
    "    yvar=yvar,  # You can change this to any other column name in your DataFrame\n",
    "    batch_size=6,\n",
    "    max_length=512\n",
    ")\n",
    "data_module.setup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/naka/code/egfr_binder_rd2/.venv/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/naka/code/egfr_binder_rd2/.venv/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/naka/code/egfr_binder_rd2/.venv/lib/python3.11 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Create the model\n",
    "model = BTRegressionModule(\n",
    "    label=yvar,\n",
    "    model_name=\"facebook/esm2_t33_650M_UR50D\",\n",
    "    lr=5e-4,\n",
    "    peft_r=8,\n",
    "    peft_alpha=16,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "# 4. Set up callbacks\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_spearman',\n",
    "    min_delta=0.00,\n",
    "    patience=30,\n",
    "    verbose=False,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# 5. Set up wandb logger\n",
    "wandb_logger = WandbLogger(project=\"bt_regression\", name=\"bt_regression_run\")\n",
    "\n",
    "# 6. Create the trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=40,\n",
    "    callbacks=[early_stop_callback],\n",
    "    logger=wandb_logger,\n",
    "    accelerator='gpu',\n",
    "    devices=1,  # Use 1 GPU\n",
    "    log_every_n_steps=10,\n",
    "    enable_checkpointing=False,  # Disable checkpoint saving\n",
    "    val_check_interval=0.25,  # Check validation 4 times per epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manaka\u001b[0m (\u001b[33manaka_personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20241025_091130-0c3nws7o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anaka_personal/bt_regression/runs/0c3nws7o' target=\"_blank\">bt_regression_run</a></strong> to <a href='https://wandb.ai/anaka_personal/bt_regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anaka_personal/bt_regression' target=\"_blank\">https://wandb.ai/anaka_personal/bt_regression</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anaka_personal/bt_regression/runs/0c3nws7o' target=\"_blank\">https://wandb.ai/anaka_personal/bt_regression/runs/0c3nws7o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 144/144 [00:00<00:00, 7681.09 examples/s]\n",
      "Map: 100%|██████████| 17/17 [00:00<00:00, 4492.95 examples/s]\n",
      "Map: 100%|██████████| 41/41 [00:00<00:00, 5835.31 examples/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                               | Params | Mode \n",
      "-----------------------------------------------------------------------------\n",
      "0 | esm_model     | PeftModelForSequenceClassification | 656 M  | train\n",
      "1 | bt_loss       | BradleyTerryLoss                   | 0      | train\n",
      "2 | train_metrics | MetricCollection                   | 0      | train\n",
      "3 | val_mae       | MeanAbsoluteError                  | 0      | train\n",
      "4 | val_spearman  | SpearmanCorrCoef                   | 0      | train\n",
      "-----------------------------------------------------------------------------\n",
      "3.7 M     Trainable params\n",
      "652 M     Non-trainable params\n",
      "656 M     Total params\n",
      "2,624.095 Total estimated model params size (MB)\n",
      "1000      Modules in train mode\n",
      "614       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naka/code/egfr_binder_rd2/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naka/code/egfr_binder_rd2/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  50%|█████     | 12/24 [00:04<00:04,  2.44it/s, v_num=ws7o]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Train the model\n",
    "trainer.fit(model, data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Test the model\n",
    "test_result = trainer.test(model, data_module)\n",
    "print(f\"Test result: {test_result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
